\documentclass[12pt]{article}
\usepackage{cite}
\usepackage{mathtools} %% includes amsmath
\usepackage{amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{physics}
\usepackage{graphicx,graphics}
\setlength{\oddsidemargin}{0.25in}
\setlength{\evensidemargin}{-0.25in}
\setlength{\topmargin}{-.5in}
\setlength{\textheight}{9in}
\setlength{\parskip}{.1in}
\setlength{\parindent}{2em}
\setlength{\textwidth}{6.25in}

\newcommand{\N}{\mathcal{N}}
\newcommand{\rv}{\vb{r}}
\newcommand{\sv}{\vb{s}}
\newcommand{\rhat}{\hat{\rv}}
\newcommand{\shat}{\hat{\sv}}

\title{Notes on b-spline implementation}
\date{\today}
\author{Robert J. Harrison}
 
\begin{document}

\maketitle

This document is initially looking at the b-spline-only implementation. 

The functional form is
\begin{eqnarray}
  f(\vb{r}) & = & \sum_{l m} \N_{lm}(\rhat) r^l R_{lm}(r) \\
  R_{lm}(r) & = & \sum_i c_{lmi} b_i(r)
\end{eqnarray}
Comments:
\begin{itemize}
\item The usual spherical coordinate system with $r = |\rv| \in [0,\infty]$, $\theta \in [0,\pi]$ with $z=r \cos \theta$, and $\phi \in [0,2 \pi]$.
\item The $\N_{lm}(\rhat)$ where $\hat{r}$ is the unit vector are the real solid spherical harmonics of Yang normalized so that
\begin{eqnarray}
  \int_0^{2 \pi} d\phi \int_0^\pi d\theta \sin \theta  \N_{lm} (\hat{r}) \N_{l^\prime m^\prime}(\hat{r})  & = & \delta_{l l^\prime} \delta_{m m^\prime}
\end{eqnarray}
Note that Yang does not normalize his harmonics for ease of computing various quantities.  However, we need radial and angular components to be normalized so we can perform numerical thresholding without having to keep track of normalization constants, which for Yang's harmonics (denoted $N_{lm}$)  can be truly huge ($O(10^{2l})$ or worse).  Yang's normalization also results in $N_{10}=z$ but $N_{11}=-x/2$ and $N_{1-1}=-y/2$ and so potentially breaks symmetry.  
\item $b_i(r)$ is a b-spline basis function defined on knots that include the origin. 
\item We separate out the factor $r^l$, with these consequences:
  \begin{enumerate}
  \item Ensures correct behavior near the origin.  If we bundled $r^l$ into $R(r)$ then then degree of the b-spline basis would have to be greater than or equal to the maximum angular momentum for an exact representation.  Since some of the algorithms scale highly non-linearly with the order, we want to use order circa 8-12.
  \item Since $r^l \N_{lm}(\rhat) = n_{lm} N_{lm}(\rv) \propto  x^i y^j z^k$ for $i+j+k=l$), we expect $R_{lm}(r)$ to tend to a non-zero constant near the origin (except for the Dirac $s$ components that are weakly singular there).  
  \item Facilitates convolution with the Coulomb and Helmholtz GFs so we can exactly cancel factors such as $r^{-l}$.
  \item Derivatives are also easier.
  \end{enumerate}
\end{itemize}

{\bf todo:} Revisit the decision to normalize the harmonics.  Maybe it is an unnecessary complication.

\section{B-splines}

Refer to standard references for details of the basis and associated algorithms.  The basis is specified by 
\begin{itemize}
\item $p$ --- the degree of the basis, and
\item a vector of unique knots.
\end{itemize}
We employ the full set of b-splines over a vector of unique knots.  The definition of the full basis requires a vector with the knots padded on either end by the last knot repeated $p$ times.  Note we actually store this vector ($t$ in the code) since it makes algorithms to compute with b-splines much easier. 

Thus, for a basis of degree $p$ (order $p+1$) and $m$ unique knots 
\begin{itemize}
\item they form (in the limit of dense knots or infinite order) a complete basis over the interval with approximation error $O\left(h^{r+p+1}\right)$ for a function with $r$ continuous derivatives 
\item the basis has $C_{p-1}$ continuity, meaning its value and all derivatives up to $p-1$ are continuous
\item the number of padded knots is $m + 2p$
\item the total number of basis functions is $m + p - 1$ 
\item the number of basis functions non-zero over a given knot interval is $p + 1$
\item each spline is non-zero over $p+1$ knot intervals and is controlled by $p+2$ knot values
\item each spline is positive within its support (and zero outside)
\item the sum of all splines at each knot is unity by construction
\item the fully interior sides of splines are zero at the end of their support along with their derivatives up to $p-1$
\item splines touching the edge have lower number of vanishing derivatives, and only the first/last splines have non-zero values at the end points (and these values will be one by construction).
\end{itemize}

\subsection{Radial knots}

We are computing on the interval $[0,L]$ which we can obtain by scaling the unit interval $[0,1]$ by $L$.  By defining monotonically-increasing maps from $[0,1]$ back onto $[0,1]$ we can nest transformations.    For $n$ knots
\begin{itemize}
\item Uniform knots $x(i) = i/(n-1)$
\item Chebyshev-like knots (that include the end points and concentrate quadratically on both ends) $x(i) = (1-\cos \pi i / (n-1))/2$
\item Half Chebyshev-like knots (that only concentrate on the left) $x(i) = 1 - \cos \pi i / (2(n-1))$
\item Rational function similar to half-Cheby $x(i) = i^2 (1+a) / (1 + ai)$
\item Geometric
\item Power
\end{itemize}

The below figures display the basis with 15 knots, order 6, with uniform and rational knots.

\includegraphics[width=.9\linewidth]{basis.pdf}

\includegraphics[width=.9\linewidth]{basis-non-uni.pdf}

\section{Radial derivatives}

The 

\section{Angular quadrature}
In double precision, the Lebedev or Beylkin rules suffice.  For higher precision, to exactly integrate angular momenta up to $L$  we employ
\begin{itemize}
\item $\theta$: The Gauss-Legendre rule of order $k_\theta = L/2 + 1$ scaled to $[0,\pi]$ ($k$ points can exactly integrate up to $x^{2k-1}$)
\item $\phi$: $k_\phi = \max(1,2 L)$ equispaced points in $[0,2 \pi)$ ($\phi_i = 2 i \pi / k_\phi$) with weights $2 \pi / k_\phi$.
 \item The total number of points $k_\theta k_\phi = (l_{\mbox{max}} + 1) \max(4 l_{\mbox{max}},1) $.
\end{itemize}
We choose $L = \max(1, 2 l_{\mbox{max}})$, where $l_{\mbox{max}}$ is the maximum angular momentum in the basis, so that we can exactly integrate products.  Table \ref{tab:GLnpts} illustrates how the required number of points scales with $l$ for each quadrature rule.

{\bf todo:} Add Beylkin's rule 

\begin{table}
  \begin{center}
  \caption{Number of angular quadrature points employed for select angular momenta} \label{tab:GLnpts}
  \begin{tabular}{ccc}
    $l_{\mbox{max}}$ & Lebedev & GL  \\ \hline
    0 & 6 & 1 \\
    1 & 6 & 8 \\
    2 & 14 & 24 \\
    3 & 26 & 48 \\
    4 & 38 & 80 \\
    5 & 50 & 120 \\
    6 & 74 & 168 \\ \hline
  \end{tabular}
  \end{center}
\end{table}

\section{Radial quadrature in the b-spline basis}

Between knots, each basis function is just a polynomial of degree $p$.  Since Gauss-Legendre (GL) quadrature with $n$ points can exactly integrate polynomials up to degree $2n-1$, these are the required number of points for various scenarios
\begin{itemize}
\item $\int r^l b_i(r) dr$ --- $n \ge (l+p-1)/2$
\item $\int r^l b_i(r) b_j(r) dr$ --- $n \ge (l+2p-1)/2$
\end{itemize}
Pre-tabulated are quadrature points and weights with $n=p+2$, which is adequate for integrals of up to the form $\int r^3 b_i(r) b_j(r)$ which is enough to construct all of the matrices needed to solve atoms.

\section{Projection}

Projecting $f(\vb{r})$ into the representation is accomplished by first integrating over the angular variables with

\begin{eqnarray}
  \int_0^{2 \pi} d\phi \int_0^\pi d\theta \sin \theta  \N_{lm} (\hat{r}) f(\vb{r}) & = & R_{lm}(r) 
\end{eqnarray}
With quadrature points on the unit sphere $\rhat_\mu$ with
weights $\omega_\mu$ and radial points $r_\nu$ , this becomes
\begin{eqnarray}
  R_{lm}(r_\nu) & = & \sum_\mu \omega_\mu \N_{lm} (\rhat_\mu) f(r_\nu \rhat_\mu)
\end{eqnarray}

The inversion from the function sampled at the grid points can be performed in several ways, paying attention to the ill-conditioning near the origin for the non-zero angular momenta.

By limiting the first (or last) $n$ basis functions the first $n$ derivatives can be forced to be zero on the left (or right).  
\begin{itemize}
\item $n=0$ no boundary conditions,
\item $n=1$ function value is forced to zero,
\item $n=2$ function value and first derivative are forced to zero,
\item etc.
\end{itemize}

\subsection{Weighted least squares and normal equations}
Adds a penalty to make the problem overall better conditioned and uses the normal equations.  For each $(l,m)$ the LSQ problem is to minimize
\begin{eqnarray}
  \sum_\nu  w(r_\nu) \left( R(r_\nu) - \sum_i c_i b_i(r_\nu) \right)^2 + \lambda \sum_i c_i^2
\end{eqnarray}
in which the weight is presumably to be chosen as $w(r)=r^2$.

The penalty term has little effect if $\lambda < \epsilon^2 / \sum_i c_i^2$. 

Setting the variation wrt $c_i$ to zero yields
\begin{eqnarray}
  0 & = & - 2 \sum_\nu  w(r_\nu) b_i(r_\nu) \left( R(r_\nu) - \sum_j c_j b_j(r_\nu) \right) + 2\lambda c_i
\end{eqnarray}
Tidying up yields
\begin{eqnarray}
  \sum_j \left( \sum_\nu w(r_\nu) b_i(r_\nu) b_j(r_\nu) \right) c_j + \lambda c_i & = & \sum_\nu  w(r_\nu) b_i(r_\nu)  R(r_\nu) \\
  \left( A + \lambda I \right) c & = & b ~ ~\mbox{with} \\
  a_{ij} & = & \sum_\nu w(r_\nu) b_i(r_\nu) b_j(r_\nu) \\
  b_i & = & \sum_\nu  w(r_\nu)  b_i(r_\nu) R(r_\nu) 
\end{eqnarray}

\subsection{Weighted least squares without using normal equations}

We can use a least-squares solver to determine $c$ in
\begin{eqnarray}
  w(r_\nu) \sum_i b_i(r_\nu) c_i \approx w(r_\nu) R(r_\nu).
\end{eqnarray}
However, it is much more convenient to construct using SVD the pseudo-inverse ($M$) of the matrix
\begin{eqnarray}
 a_{\nu i} & = & b_i(r_\nu) w(r_\nu) 
\end{eqnarray}
The b-spline basis is strongly linearly independent so there so there should always be a clear divide between zero and non-zero singular values and you can just use the expected number of non-zero singular values.

The fit is obtained directly with a matrix-vector product of $M$ acting on the vector of weighted function values evaluated at the (over-sampled) radial points.

\subsection{Projection into the b-spline basis}

Starting from 
\begin{eqnarray}
  \sum_j b_j(r) c_j \approx R(r)
\end{eqnarray}
we project from the left by $b_i(r)$ to obtain
\begin{eqnarray}
  \sum_j \left\langle b_i \middle| b_j\right\rangle c_j & \approx & \left\langle b_i \middle| R \right\rangle
\end{eqnarray}
The matrix on the left is just the overlap matrix ($S$) and we expand the integrals on the right using GL quadrature
\begin{eqnarray}
\left\langle b_i \middle| R \right\rangle = \sum_\nu b_i(r_\nu) \omega_\nu  R(r_\nu)
\end{eqnarray}
Hence, the fitting matrix ($M$) is given by
\begin{eqnarray}
   M & = & S^{-1} B
\end{eqnarray}
with
\begin{eqnarray}
  b_{i \nu} & = & b_i(r_\nu) \omega_\nu 
\end{eqnarray}

\section{Convolution with an integral operator}

Angular part is diagonal, so focus here on the radial part.

\section{Solid harmonics}

The functions $\N_{lm}(\vb{r})$ are normalized versions of the solid harmonics of Yang ($N_{lm}$).  The unnormalized versions are defined by the recursions (with $m\ge 0$) (the first two are used to recur up the diagonal $m=\pm l$ and the next two are then used to recur $l$ down from $l=m$ to $l=0$ (I inserted commas in the subscripts for clarity)
\begin{eqnarray}
  N_{0,0} & = & 1 \\
  N_{l, l} & = & -\frac{1}{2 l} \left(x N_{l-1,l-1} - y N_{l - 1, -(l - 1)}\right) \\
  N_{1,-l} & = & -\frac{1}{2 l} \left(y N_{l-1,l-1} + x N_{l - 1, -(l - 1)}\right) \\ 
  N_{l, m} & = & \frac{1}{(l+m) (l-m)} \left((2 l-1) z N_{l-1, m}-r^2  N_{l-2, m} \right) \\
  N_{l, -m}& = & \frac{1}{(l+m) (l-m)} \left((2 l-1) z N_{l-1, -m}-r^2 N_{l-2, -m} \right) 
\end{eqnarray}
with normalization constants
\begin{eqnarray}
  n_{lm} & = & \frac{\sqrt{(2 l +1) \left(l +{| m |}\right)! \left(l -{| m |}\right)!}  2^{-(1 + \delta_{0 m})/2}}{\sqrt{\pi}} .
\end{eqnarray}
The normalized harmonics are then
\begin{eqnarray}
  \N_{lm}(\vb{r}) & = & n_{lm} N_{lm}(\vb{r}).
\end{eqnarray}
The normalization is in the sense that an integral over the unit sphere yields
\begin{eqnarray}
  \int_0^{2 \pi} d\phi \int_0^\pi d\theta \sin \theta  \N_{lm} (\hat{r}) \N_{l^\prime m^\prime}(\hat{r})  & = & \delta_{l l^\prime} \delta_{m m^\prime} .
\end{eqnarray}
We tabulate the normalization constants and their reciprocals, so we can internally use the simple formulae of the unnormalized harmonics.

Note that $N_{lm}(\rv) = r^l \N_{lm}(\rhat)$.

The first few unnormalized solid harmonics are
\begin{eqnarray}
N_{0, 0} & = &  1  \nonumber \\
N_{1, -1} & = &  -\frac{y}{2}  \nonumber \\
N_{1, 0} & = &  z  \nonumber \\
N_{1, 1} & = &  -\frac{x}{2}  \nonumber \\
N_{2, -2} & = &  \frac{y x}{4}  \nonumber \\
N_{2, -1} & = &  -\frac{z y}{2}  \nonumber \\
N_{2, 0} & = &  -\frac{r^{2}}{4}+\frac{3 z^{2}}{4}  \nonumber \\
N_{2, 1} & = &  -\frac{z x}{2}  \nonumber \\
N_{2, 2} & = &  \frac{x^{2}}{8}-\frac{y^{2}}{8}  \nonumber \\
N_{3, -3} & = &  -\frac{1}{16} x^{2} y +\frac{1}{48} y^{3}  \nonumber \\
N_{3, -2} & = &  \frac{z y x}{4}  \nonumber \\
N_{3, -1} & = &  \frac{y \left(r^{2}-5 z^{2}\right)}{16}  \nonumber \\
N_{3, 0} & = &  -\frac{1}{4} r^{2} z +\frac{5}{12} z^{3}  \nonumber \\
N_{3, 1} & = &  \frac{x \left(r^{2}-5 z^{2}\right)}{16}  \nonumber \\
N_{3, 2} & = &  \frac{1}{8} z \,x^{2}-\frac{1}{8} z \,y^{2}  \nonumber \\
N_{3, 3} & = &  -\frac{1}{48} x^{3}+\frac{1}{16} y^{2} x  \nonumber
\end{eqnarray}

The first few normalized solid harmonics are
\begin{eqnarray}
\N_{0, 0} & = &  \frac{1}{2 \sqrt{\pi}}  \nonumber \\
\N_{1, -1} & = &  -\frac{\sqrt{3} y}{2 \sqrt{\pi}} \nonumber \\
\N_{1, 0} & = &  \frac{\sqrt{3} z}{2 \sqrt{\pi}} \nonumber \\
\N_{1, 1} & = &  -\frac{\sqrt{3} x}{2 \sqrt{\pi}} \nonumber \\
\N_{2, -2} & = &  \frac{\sqrt{15} y x}{2 \sqrt{\pi}} \nonumber \\
\N_{2, -1} & = &  -\frac{\sqrt{15} z y}{2 \sqrt{\pi}} \nonumber \\
\N_{2, 0} & = &  -\frac{\sqrt{5} \left(r^{2}-3 z^{2}\right)}{4 \sqrt{\pi}} \nonumber \\
\N_{2, 1} & = &  -\frac{\sqrt{15} z x}{2 \sqrt{\pi}} \nonumber \\
\N_{2, 2} & = &  \frac{\sqrt{15} \left(x^{2}-y^{2}\right)}{4 \sqrt{\pi}} \nonumber \\
\N_{3, -3} & = &  -\frac{3 \sqrt{70} y \left(x^{2}-\frac{y^{2}}{3}\right)}{8 \sqrt{\pi}} \nonumber \\
\N_{3, -2} & = &  \frac{\sqrt{105} z y x}{2 \sqrt{\pi}} \nonumber \\
\N_{3, -1} & = &  \frac{y \left(r^{2}-5 z^{2}\right) \sqrt{42}}{8 \sqrt{\pi}} \nonumber \\
\N_{3, 0} & = &  -\frac{3 \sqrt{7} z \left(r^{2}-\frac{5 z^{2}}{3}\right)}{4 \sqrt{\pi}}  \nonumber \\
\N_{3, 1} & = &  \frac{x \left(r^{2}-5 z^{2}\right) \sqrt{42}}{8 \sqrt{\pi}} \nonumber \\
\N_{3, 2} & = &  \frac{\sqrt{105} z \left(x^{2}-y^{2}\right)}{4 \sqrt{\pi}} \nonumber \\
\N_{3, 3} & = &  -\frac{x \left(x^{2}-3 y^{2}\right) \sqrt{70}}{8 \sqrt{\pi}} \nonumber
\end{eqnarray}

\subsection{Products of solid harmonics}

The addition of two angular momenta $l_1$ and $l_2$ (i.e., the product of the two functions) in general is a superposition of angular momenta $l_3 \in [|l_1-l_2|,l_1+l_2]$ and with $m_3 = m_1 + m_2$ (for the solid harmonics this becomes $m_3 = \pm m_1 \pm m_2$).  Rather than mess with Wigner 3j symbols or Clebsch-Gordon coefficients, etc., we just brute force compute the non-zero coefficients using orthogonal projection using Gauss-Legendre quadrature in quad-double arithmetic to ensure all significant figures are correct for single, double, or double-double precision arithmetic, and all but the last 2-4 bits are correct in quad-double.

Defining 
\begin{eqnarray}
  \N_{l_1 m_1}(\rhat) \N_{l_2 m_2}(\rhat) & = & \sum_{l_3 m_3} j_{l_1, m_1, l_2, m_2, l_3, m_3} \N_{l_3 m_3}(\rhat)
\end{eqnarray}
and projecting from the left by $\N_{l_3 m_3}(\rhat)$ we obtain (noting the normalization condition)
\begin{eqnarray}
  j_{l_1, m_1, l_2, m_2, l_3, m_3} = \int_0^\pi d\theta \sin \theta \int_0^{2 \pi} d\phi \N_{l_1 m_1}(\rhat) \N_{l_2 m_2}(\rhat) \N_{l_3 m_3}(\rhat) .
\end{eqnarray}
Non-zero values are tabulated and stored in a file along with an index vector to facilitate look up via $(l_1, m_1, l_2, m_2)$, using the permutation symmetry between the first two indices, the restrictions $l \ge |m|$, and with the range of $l_3$ being double that of $l_1$ and $l_2$.

With $L$ as the maximum value of both $l_1$ and $l_2$, we have $l_3 \le 2 L$, the size of the index vector is $(L+1)^4$ with the offset into the index vector computed using
\begin{eqnarray}
  (l_1, m_1, l_2, m_2) & \rightarrow & (l_1 (l_1+1)+m_1) (L+1)^2 + l_2 (l_2+1)+m_2 .
\end{eqnarray}
The index vector stores the offset into the linear array of coupling coefficients and the number of entries.  The linear table of coefficients stores $l_3$, $m_3$ and the coefficients.
The number of non-zero coefficients for $L \ge 8$ is tightly bounded from above by $L (L+1)^4 / 2$. For $L=20$ this is less than 2M, and for $L=12$ is less than 200K.

For derivatives, we must compute products with unit vectors in the Cartesian directions, and so need the coupling coefficients $j_{1, 0, l, m, l\pm 1, m}$ and $j_{1, \pm 1, l, m, l\pm 1, m\pm 1}$.  For examples, see table \ref{tab:3j10}.
\begin{table}
  \caption{Example 3-j coupling coeffs for $\N_{10}$ times $\N_{lm}$.} \label{tab:3j10}
  \begin{center}    
  \begin{tabular}{lcll}
$\N_{00}$  &   $\rightarrow$ &  $ 1/\sqrt{4 \pi} \N_{10} $ +  $\pi/3 \N_{10}$\\
$\N_{1 -1}$&   $\rightarrow$ &  $1/\sqrt{\pi (7-1/3)} \N_{2 -1}$  \\
$\N_{10}$ & $\rightarrow$ &  $1/\sqrt{4 \pi}  \N_{0 0} +   1/\sqrt{5 \pi} \N_{2 0}$ \\
$\N_{11}$ &  $\rightarrow$ &   $1/\sqrt{\pi (7-1/3)} \N_{2 1}$ \\
$\N_{2-2}$& $\rightarrow$ &  $1/\sqrt{\pi (9+1/3)} \N_{3 -2}$ \\
$\N_{2-1}$& $\rightarrow$ &  $1/\sqrt{\pi (7-1/3)} \N_{1 -1} +  1/\sqrt{\pi (6-1/6)} \N_{3 -1} $
  \end{tabular}
  \end{center}
\end{table}

These have simpler forms in the unnormalized basis, for instance (noting we evaluate the below at $r=1$)
\begin{eqnarray}
z N_{lm} = \frac{\left(\left(l +1\right)^{2}-m^{2}\right) \mathit{Np}_{l +1,m}+r^{2} \mathit{Np}_{l -1,m}}{2 l +1} .
\end{eqnarray}
However, between the sparsity pattern and still having to tabulate factorials, etc.,  it seems easier for now to just use the numerically computed coupling coefficients and associated index vector.  The lookup will be outside the innermost loop(s) so should not impact performance.

The coupling coefficients in the unnormalized basis $\bar{j}$ , defined so that
\begin{eqnarray}
  N_{l_1 m_1}(\rv) N_{l_2 m_2}(\rv) & = & \sum_{l_3 m_3} \bar{j}_{l_1, m_1, l_2, m_2, l_3, m_3} N_{l_3 m_3}(\rv)  
\end{eqnarray}
can be computed from those in the normalized basis ($j$) as follows
\begin{eqnarray}
  \bar{j}_{l_1, m_1, l_2, m_2, l_3, m_3} & = & j_{l_1, m_1, l_2, m_2, l_3, m_3} n^{-1}_{l_1 m_1} n^{-1}_{l_2 m_2} n_{l_3 m_3} .
\end{eqnarray}
We note that while $j$ is symmetric wrt exchange of all three pairs of indices, $\bar{j}$ is only symmetric wrt exchange of the first two.


\subsection{Derivatives of solid harmonics}
\label{sec:solder}

Derivatives of the unnormalized harmonics are computed as follows with $m\ge 0$, treating as zero $N_{lm}$ with $|m|>l$, and again using commas for clarity
\begin{eqnarray}
  \frac{\partial}{\partial x} N_{l,\pm m} & = & \frac{1}{2}\left( N_{l-1,\pm (m+1)} - N_{l-1,\pm (m-1)} \right) \nonumber \\
  \frac{\partial}{\partial y} N_{l,\pm m} & = & \pm \frac{1}{2}\left( N_{l-1,\mp (m+1)} + N_{l-1,\mp (m-1)} \right) \nonumber \\
  \frac{\partial}{\partial z} N_{l,\pm m} & = & N_{l-1,\pm m} . \nonumber
\end{eqnarray}

\section{Derivatives of functions in the combined basis}

Rewriting our representation in terms of the non-normalized spherical harmonics
\begin{eqnarray}
  f(\rv) & = & \sum_{l m} n_{lm} N_{lm}(\rv) R_{lm}(r) ,
\end{eqnarray}
we employ the above results of derivatives of the solid harmonics to obtain (for some Cartesian direction $q$)
\begin{eqnarray}
  \frac{\partial}{\partial q} f(\rv) & = & \sum_{l m} n_{lm} \left(R_{lm}(r) \frac{\partial}{\partial q} N_{lm}(\rv)  + N_{lm}(\rv)  \frac{\partial}{\partial q} R_{lm}(r) \right) \\
   & = & \sum_{l m} n_{lm} \left( R_{lm}(r) \frac{\partial}{\partial q} N_{lm}(\rv)  + N_{lm}(\rv) \hat{\vb{q}} \frac{d}{dr} R_{lm}(r) \right).
\end{eqnarray}
Also from above we see that the Cartesian components of the unit vector $\rhat$ are
\begin{eqnarray}
  x & = & - \frac{2 \sqrt{\pi}}{\sqrt{3}} \N_{1,1} = -2 N_{11}\\
  y & = & - \frac{2 \sqrt{\pi}}{\sqrt{3}} \N_{1,-1}= -2 N_{1-1} \\
  z & = & + \frac{2 \sqrt{\pi}}{\sqrt{3}} \N_{1,0} = N_{10}.
\end{eqnarray}
The necessary coupling cofficients ($j_{1, 0, l, m, l\pm 1, m}$ and $j_{1, \pm 1, l m l\pm 1, m\pm 1}$) are discussed above.

The algorithm for differentiation starts by pre-tabulating the required unnormalized coupling coeffcients which are of size $O(L^2)$.
\begin{enumerate}
\item Scale $R_{lm}$ by $n_{lm}$. 
\item Perform the differentiation of the angular part by combining the radial parts according to the expressions in section \ref{sec:solder}
\item Differentiate the radial part and add into the appropriate components of the result scaled by the coupling coefficients.
\item With a little more effort these two steps could be combined and perhaps even turned into a matrix-matrix operation.
\end{enumerate}

\section{Bound-state Helmholtz Green's function}

The 3D Green's function (GF) satisfies
\begin{eqnarray}
   \left(-\nabla^2 + \mu^2\right) G(\rv,\sv; \mu) & = & \delta^{(3)}(\rv - \sv)
\end{eqnarray}
and is given by
\begin{eqnarray}
  G(\rv,\sv; \mu) & = & \frac{\mathrm{e}^{-\mu |\rv-\sv|}}{4 \pi |\rv-\sv|} .
\end{eqnarray}
In the limit $\mu=0$ the Coulomb GF is obtained.

Expanding using the normalized solid harmonics
\begin{eqnarray}
  G(\rv,\sv; \mu) & = & \mu 
  \sum_{lm}
  \left(    
  \Theta(r-s) h_l(\mu r) j_l(\mu s)
  +
  \Theta(s-r) j_l(\mu r) h_l(\mu s)
  \right)
  \N_{lm}(\rhat) \N_{lm}(\shat) .
\end{eqnarray}
Note the radial part of the kernel is independent of $m$.

The Coulomb GF is obtained in the limit $\mu=0$, which here includes in the angular part of the integration the $1/4\pi$ unlike elsewhere in MADNESS.  The factor of $\mu$ at the front is cancelled, so just set it to one for this purpose.
\begin{eqnarray}
  h_l(r) & = & r^{-(l+1)} , \\
  j_l(r) & = & \frac{r^l}{2l+1}.
\end{eqnarray}

For the BSH, the real functions $h_l$ (modified spherical Hankel functions of the first kind) and $j_l$ (modified spherical Bessel functions of the first kind) are defined as
\begin{eqnarray}
  h_l(r) & = & -\sqrt{\frac{\pi}{2r}} {\mathrm e}^{i \left(l-\frac{1}{2}\right) \frac{\pi}{2}} H_{l +\frac{1}{2}}^{\left(1\right)} \left(i r  \right) ,\\
  j_l(r) & = & \sqrt{\frac{\pi}{2r}} {\mathrm e}^{-i \left(l+\frac{1}{2}\right) \frac{\pi}{2}} J_{l +\frac{1}{2}} \left(i r  \right) .
\end{eqnarray}
where $H^{(1)}$ and $J$ are modified (i.e., imaginary argument) Hankel and Bessel functions, respectively, of the first kind.

The functions $h_l(r)$ are everywhere positive and decay exponentially at long range,
\begin{eqnarray}
  h_0(r) & = & \frac{{\mathrm e}^{-r}}{r}, \\
  h_1(r) & = & \frac{{\mathrm e}^{-r} \left(r +1\right) }{r^{2}} ,\\
  h_2(r) & = & \frac{{\mathrm e}^{-r} \left(r^{2}+3 r +3\right)}{r^{3}} ,\\
  h_3(r) & = & \frac{{\mathrm e}^{-r} \left(r^{3}+6 r^{2}+15 r +15\right)}{r^{4}} .
\end{eqnarray}
We also have
\begin{eqnarray}
  \lim_{r \rightarrow 0} h_l(r) & = & \frac{(2l-1)!!}{r^{l+1}} , \\
  \lim_{r \rightarrow \infty} h_l(r) & = & \frac{{\mathrm e}^{-r}}{r} .
\end{eqnarray}

The functions $j_l(r)$ are also everywhere positive but have exponentially growing and decaying components,
\begin{eqnarray}
  j_0(r) & = & \frac{{\mathrm e}^{r}-{\mathrm e}^{-r}}{2 r} , \label{eqn:j0} \\
  j_1(r) & = & \frac{{\mathrm e}^{-r}\left(r +1\right) +{\mathrm e}^{r} \left(r -1\right)}{2 r^{2}} , \\
  j_2(r) & = & \frac{{\mathrm e}^{-r}\left(-r^{2}-3 r -3\right)+{\mathrm e}^{r} \left(r^{2}-3 r +3\right)}{2 r^{3}} , \\
  j_3(r) & = & \frac{{\mathrm e}^{-r}\left(r^{3}+6 r^{2}+15 r +15\right)+{\mathrm e}^{r} \left(r^{3}-6 r^{2}+15 r -15\right)}{2 r^{4}} .
\end{eqnarray}
We also have
\begin{eqnarray} 
  \lim_{r \rightarrow 0} j_l(r) & = & \frac{r^l}{(2l+1)!!} , \label{eqn:jsmallr}\\
  \lim_{r \rightarrow 0} \frac{j_l(r)}{j_{l-1}} & = & \frac{r}{2l+1} \\
  \lim_{r \rightarrow \infty} j_l(r) & = & \frac{{\mathrm e}^{r}}{2r} \\
  \lim_{r \rightarrow \infty} \frac{j_l(r)}{j_{l-1}} & = & 1 - \frac{l}{r} + \frac{l(l-1)}{2 r^2} + \frac{l(l-1)}{2 r^3} - O(r^{-4}) .
\end{eqnarray}
Note that the asymptotic behavior won't dominate until for large $r$ we have $r \gg l$, and for small $r$ that $r \ll 2l+1$.  The large $l$ limit emerges from the Taylor series expansion given further below, or from the expression above for the small $r$ limit (i.e., small relative to $l$).

Recall that $\mu = \sqrt{-2 \epsilon}$ and for heavy elements is in the approximate range $0.1<\mu<100$.  Also, our simulation box is circa 100 units and we need to resolve close to the singularity if computing with a point charge model.  Thus, we will need to evaluate with something like $10^{-7} < \mu r < 10^4$.  Note that $r^{30}exp(-r)$ is less than 1e-16 for $r<0.295$ and $r>195$.

Derivatives.  Following from the NIST Handbook section 10.51(ii) we get things that look like backward, central, and foward difference formulas
\begin{eqnarray}
  j_l(r)^\prime & = & j_{l-1} - \frac{n+1}{r} j_l(r) \\
              & = & \frac{1}{2l+1} \left( l j_{l-1}(r) + (l+1) j_{l+1}(r)  \right) \\
              & = &  j_{l+1} + \frac{n}{r} j_l(r)
\end{eqnarray}


Underflow, overflow, and cancellation all need to be explicitly managed when evaluating these functions.
\begin{itemize}
\item  Underflow: Near the origin, $j_l$ can underflow and be rounded to zero.  But, it can be multiplied by a corresponding large $h_l$ that if $j_l$ were correct would product a significant contribution.  At long range, $h_l$ gets very small and can underflow, but can be multiplied by an exponentially large $j_l$.
\item Overflow: We've already noted that $h_l$ can get big and overflow near the origin.  Similarly, $j_l$ gets exponentially large at long range and will easily overflow.
\item Cancellation: The upward recurrence to evaluate $j_l$ is unstable and is discussed below.
\item Cancellation: Even the expression for $j_1(r)$ is unstable for $r<0.02$ (in double precision) since it limits to $O(r^2)$ and even using \verb+std::expm1+ only corrects the linear term in the cancellation.
\item Exponent range: In double precision $10^{\pm 308}$.  In single precision $10^{\pm 38}$.  Too positive will give overflow; too negative will initially produce denormalized numbers (reduced precision) and then underflow to zero.
\end{itemize}

For these reasons, instead of computing the bare functions, we examined computing with
\begin{eqnarray*}
  \bar{h}_l & = & {\mathrm e}^{-r} \frac{r^{l+1}}{(2l-1)!!} h_l(r) ,\\
  \bar{j}_l & = & {\mathrm e}^{r} \frac{(2l+1)!!}{r^l} j_l(r).
\end{eqnarray*}
However, the additional powers of $r$ were not that helpful (basically just moved the problems from one end of the range to the other) and make other aspects of the calculation more complicated.  So we instead just explicitly separate out the exponential with
\begin{eqnarray}
  \bar{h}_l & = & {\mathrm e}^{-r} h_l(r) , \\
  \bar{j}_l & = & {\mathrm e}^{r} j_l(r) .
\end{eqnarray}
The asymptotics at the origin are unchanged, and at long range are now
\begin{eqnarray}
  \lim_{r \rightarrow \infty} \bar{h}_l(r) & = & \frac{1}{r}, \\
  \lim_{r \rightarrow \infty} \bar{j}_l(r) & = & \frac{1}{2r} .
\end{eqnarray}

In terms of these functions, the BSH GF becomes
\begin{eqnarray}
  G(\rv,\sv; \mu) & = & \mu {\mathrm e}^{-\mu |r-s|}
  \sum_{lm} \\
  & & 
  \left(    
  \Theta(r-s) \bar{h}_l(\mu r) \bar{j}_l(\mu s)
  +
  \Theta(s-r) \bar{j}_l(\mu r) \bar{h}_l(\mu s)
  \right)
  \N_{lm}(\rhat) \N_{lm}(\shat)
\end{eqnarray}
in which, due to the Heaviside functions, we can now see that the exponential is always decaying.

\subsection{Recursions to evaluate the spherical Bessel/Hankel functions}

The relevant recursions for $\bar{h}_l(r)$ and $\bar{j}_l(r)$, as defined above, are the same as those for $h_l(r)$ and $j_l(r)$, differing only in their starting values.
\begin{eqnarray}
  \bar{j}_{l+1} & = & -\frac{2l+1}{r} \bar{j}_l + \bar{j}_{l-1} \\
  \bar{j}_{l-1} & = & \!\ \frac{2l+1}{r} \bar{j}_l + \bar{j}_{l+1} \\
  \bar{h}_{l+1} & = & \!\ \frac{2l+1}{r} \bar{h}_l + \bar{h}_{l-1} \\
  \bar{j}_0(r) & = & -\frac{\mathrm{expm1}(-2r)}{2r} \\
  \bar{j}_1(r) & = & \frac{r-1+\exp(-2r)(r+1)}{2 r^2} \\
  \bar{h}_0(r) & = & \frac{1}{r} \\
  \bar{h}_1(r) & = & \frac{r+1}{r^2}
\end{eqnarray}
The upward recursion for $\bar{h}_l$ is stable for all $r$.  The downward recursion for $\bar{j}$ is stable for all $r$, and Miller's algorithm can be used reliably, but it is expensive for large $r$ (since the downward recursion must be started at very high $l$).  The upward recursion for $\bar{j}_l$ is unstable, but for large $r$ it can provide sufficient accuracy, even without extended precision.  With extended precision, it is possible to evaluate the entire range --- this is examined in the following table.

The Taylor series for $j_l$ is rapidly convergent ($j$ instead of $\bar{j}$ is preferable since the former contains only even/odd terms)
\begin{eqnarray}
  j_l(r) & = & r^l \sum_{k=0}^\infty \frac{r^{2k}}{2^k k! (2l+2k+1)!!} \\
  & = & \frac{r^l}{(2l+1)!!} \left(1  + \frac{r^2}{2 (2l+3)} + \frac{r^4}{8 (2l+3)(2l+5)} + \cdots \right)
\end{eqnarray}

\subsection{Integral representation}

There is a curious integral representation for $j_l(r)$ (there's a phase factor missing here --- prolly $-1^l$)
\begin{eqnarray}
  j_l(r) & = & \frac{1}{2} \int_{-1}^1 dx \exp(-xr) P_l(x) .
\end{eqnarray}
[Aside: this perhaps relates to the the asymptotic formula for GL quadrature points and weights].
    
Thinking about orthogonal projection, we can see that $j_l(r)$ is just the $l$'th coefficient in the expansion of $\exp(-xr)$ in terms of Legendre polynomials.  The orthogonality of $P_l$ to lower order monomials implies that the first $l$ terms in the Taylor series of $\exp(-xr)$ do not contribute.  Taking this into account is essential for small $r$ or large $l$.  The necessary order of the GL quadrature must exactly integrate the kernel of the integral assuming that the Taylor series expansion of the exponential is terminated at some degree $k$.  Thus, the number of quadrature points $n \ge (l+k+1)/2$.

\subsection{Miller's algorithm}

The upward recursion can be seen to be unstable from at least two perspectives.  The first is cancellation since in $j_l$ the first $l$ terms in the Taylor series vanishes.  Even evaluating $j_0$ (equation \ref{eqn:j0}) must be done with care for small $r$. However, this hints that the upward recursion can work for large $r$.   The second is that there are two solutions to the upward recursion --- one growing expoenntially and the other, which is the one we want. Any numerical error will rapidly overwhelm the desired solution.  Google for ``linear second-order recurrence'' to learn more.

Equation \ref{eqn:jsmallr} (the first term in the Taylor series) is also the asymptotic form of $j_l$ for large order $l$.  Recalling the gamma function at half-integral arguments and Stirling's formula,
\begin{eqnarray}
  \Gamma(n+\frac{1}{2}) & = & 2^{-n} \Gamma(\frac{1}{2}) (2n-1)!! \\
  \Gamma(\frac{1}{2}) & = & \sqrt{\pi} \\
  \Gamma(z) & \approx & \sqrt{2 \pi} z^{z - \frac{1}{2}} \exp(-z) \\
\end{eqnarray}
we obtain
\begin{eqnarray}
  n! & \rightarrow & \sqrt{2 \pi} n^{n+1/2} e^{-n} \\
  (2n+1)!! & = & \frac{(2n+1)!}{2^n n!} \\
  & \rightarrow & (2n + 1)^{2n+3/2} \exp(-n - 1) n^{-n - 1/2} 2^{-n}
\end{eqnarray}
which are accurate to a few percent even for $n=1$.  Thus, 
\begin{eqnarray}
  j_l(r) & \rightarrow & \sqrt{2} (2 l+3)^{l+1} e^{-l-\frac{3}{2}} r^l 
\end{eqnarray}

\subsubsection{How to chose the upper limit in the downward recursion?}

To avoid overflow in the downward recursion we already rewrite the original downward recursion
\begin{eqnarray}
  j_{n-1} & = & j_{n+1} + \frac{2n+1}{r} j_{n} 
\end{eqnarray}
to compute the ratio
\begin{eqnarray}
  R_n & = & \frac{j_n}{j_{n-1}} \\
      & < & 1 ~ ~ \forall ~ ~ r>0.
\end{eqnarray}
yielding
\begin{eqnarray}
  R_{n} & = & \frac{1}{R_{n+1} + \frac{2n+1}{r}}.
\end{eqnarray}
This connects us to the continued-fraction form of the solution to the recurrence.

Note for large $n$ s.t. $2n+1 \gg r$ (small $r$ limit)
\begin{eqnarray}
  R_n & \rightarrow & \frac{r}{2n+1} - \frac{r^3}{(2n + 3)(2 n + 1)^2} + \frac{2r^5}{(2 n + 5) (2 n + 3) (2 n + 1)^3} - \cdots
\end{eqnarray}
and for small $n$ s.t. $n \ll r$ (large $r$ limit)
\begin{eqnarray}
  \lim_{r \rightarrow \infty} R_n & = & 1 - \frac{n}{r} + \frac{n(n-1)}{2 r^2} + \frac{n(n-1)}{2 r^3} - O(r^{-4}).
\end{eqnarray}

Examining the original downward recurrence for $j_n$ and starting it with positive values, then all $j_n(r)>0$.  Hence, $j_{n-1}(r) > j_{n+1}(r)$ or $R_n R_{n+1} < 1$.  But there is likely a stronger statement that $1 > R_n > R_{n+1}$ for all $n$ and $r$ --- empirically this is true.  The asymptotic forms of $j_n(r)$ for either small or large $r$ yield this result in those regions.  What happens in the middle ground? The ratio $R_n(r=2n+1)$ varies only slightly from $R_2=0.65$ to $R_{30}=0.62$.  Similarly, $R_n(r=n)$ yields $R_2=0.36$ and $R_{30}=.41$.  Thus, there are nearly universal (independent of $n$) values of the ratio in this transition zone.

What are sensible constraints on the starting guess?  We would naturally pick $R_{n+1}=0$, which results in $R_n = r/(2n+1)$ but then we would have $R_n < 1$ only in the small $r$ zone.  For this, our initial guess should be adjusted to $R_{n+1} > \max (0,1 - (2n+1)/r)$.  To satisfy the tighter $R_n < R_{n+1}$ we pick our guess $\bar{R}_{n+1}$
\begin{eqnarray}
  \bar{R}_{n+1} & = & \frac{-\frac{2n+3}{r} + \sqrt{\left(\frac{2n+3}{r}\right)^2 + 4}}{2}
\end{eqnarray}
We get lucky!  This formula does a remarkably good job of estimating the ratio for all values of $r$. Part of this is that it builds in nearly the correct limits for small ($R_n \rightarrow 0$) and large ($R_n \rightarrow 1$) $r$ and monotonically interpolates between them.
\begin{eqnarray}
  \lim_{r \rightarrow 0} \bar{R}_{n}(r) & = & \frac{r}{2 n + 1} - \frac{r^3}{(2n + 1)^3} + \cdots \\
  \lim_{r \rightarrow \infty} \bar{R}_{n}(r) & = & 1 - \frac{2n - 1}{2r} + \frac{(2n + 1)^2}{8 r^2} + \cdots
\end{eqnarray}

With this insight, we can solve for rational functions that reproduce the first two terms in both the Taylor series at the origin and asymptotic series for large $r$
\begin{eqnarray}
  \bar{R}_n & = & \frac{r^2 + (n + 1)r}{r^2 + (2 n + 1) r + (2 n + 1) (n + 1)}
\end{eqnarray}
or the first three terms of each
\begin{eqnarray}
  \bar{R}_{n} & = & \frac{2 r^{3}+\left(3 n +3\right) r^{2}+\left(2 n +3\right) \left(n +1\right) r}{2 r^{3}+\left(5 n +3\right) r^{2}+\left(6 n +3\right) \left(n +1\right) r +\left(2 n +3\right) \left(2 n +1\right) \left(n +1\right)}
\end{eqnarray}
The first of these is pleasingly simple but is better only at small $r$, whereas the latter is better over the entire range --- but not for larger $l$.  So we'll probably just stick with the original version.

If we make an error $\epsilon_{n+1}$ in $R_{n+1}$ (so $R_{n+1} = \bar{R}_{n+1} + \epsilon_{n+1}$), we can estimate the error in $R_{n}$ as follows
\begin{eqnarray}
  R_{n} + \epsilon_{n} & = & \frac{1}{R_{n+1} + \epsilon_{n+1} + \frac{2n+1}{r}} \\
  & = & \frac{1}{\left(R_{n+1} + \frac{2n+1}{r}\right) \left(1 + \epsilon_{n+1}\frac{1}{R_{n+1} +  \frac{2n+1}{r}} \right)} \\
  & = & R_{n} \frac{1}{1 + \epsilon_{n+1} R_{n}} \\
  & \approx & R_{n} - \epsilon_{n+1} R^2_n 
\end{eqnarray}
in the last line of which we assumed that 
\begin{eqnarray}
  \epsilon_{n+1} R_{n} & \ll & 1 .
\end{eqnarray}
Hence,
\begin{eqnarray}
  \epsilon_n & \approx & \epsilon_{n+1} R^2_n
\end{eqnarray}
This result agrees excellently with numerical experiments, and also with Perron's theorem that describes how the ratio between the two solutions changes between iterations of the recurrence.

We want full accuracy in $R_l$ since it connects $j_l$ to $j_{l-1}$, etc., and we will compute the scaling factor from $j_0$.
Recurring down from $n+1$ (which we estimate from one of the above formulae) to $l$, the error in $R_l$ is
\begin{eqnarray}
  \epsilon_l & \approx & \epsilon_{n+1} R^2_n R^2_{n-1} \ldots R^2_l \\
             & = & \epsilon_{n+1} \prod_{m=l}^n R^2_m
\end{eqnarray}
and hence the relative error in $R_l$ is
\begin{eqnarray}
  \epsilon_l R_l^{-1} & \approx & \epsilon_{n+1} R_l \prod_{m=l+1}^n R^2_m
\end{eqnarray}

Defining $\gamma$ to be the geometric mean of the relevant ratios so that
\begin{eqnarray}
   \prod_{m=l+1}^n R^2_m  & = & \gamma^{2(n-l)}
\end{eqnarray}
we obtain
\begin{eqnarray}
  \epsilon_l R_l^{-1} & \approx & \epsilon_{n+1} R_l \gamma^{2(n-l)}
\end{eqnarray}

So for a relative error of $p$ decimal places, we can estimate the required $n$ by solving
\begin{eqnarray}
  -2.3 p & \approx  & \log \epsilon_{n+1} + \log R_l + 2(n-l) \log \gamma
\end{eqnarray}
Approximating $\gamma = \sqrt{\bar{R}_l \bar{R}_{n+1}}$, noting we already need to compute $\bar{R}_{n+1}$ for the starting guess, we obtain
\begin{eqnarray}
  -2.3 p & \approx  & \log \epsilon_{n+1} + \log R_l + (n-l) \log \bar{R}_l \bar{R}_{n+1}
\end{eqnarray}

Still need estimate of $\epsilon_{n+1}$ !!!!!!!!!!!!

\subsubsection{When is the upward recursion stable?}

Starting from the upward recursion for the ratio
\begin{eqnarray}
  R_{n+1} & = & R_{n}^{-1} - \frac{2n+1}{r}
\end{eqnarray}
we first examine the behavior for small $r$.  The series expansion for $R_n(r)$ yields
\begin{eqnarray}
  R_n^{-1} & = & \frac{2n+1}{r} \left(1 + \frac{r^2}{(2n+1)(2n+3)} + \cdots \right) .
\end{eqnarray}
hence,
\begin{eqnarray}
  R_{n+1} & = & \frac{2n+1}{r} \left(1 + \frac{r^2}{(2n+1)(2n+3)} + \cdots - 1 \right) \\
         & = & \frac{r}{2n+3} + \cdots
\end{eqnarray}
with the $1$ on the far right being from the subtraction in the recurrence, and the second line assuming exact arithmetic and showing how the recursion follows from the Taylor series expansion, or vice versa. Introducing a relative error $\epsilon_n$ in the computed value for $R_n$ and also the relative precision of computation $\epsilon_{\mbox{mach}}$, we obtain
\begin{eqnarray}
  R_{n+1} & = & \frac{2n+1}{r} \left( (1+\epsilon_n) \left(1 + \frac{r^2}{(2n+1)(2n+3)} + \cdots\right) - 1 + \epsilon_{\mbox{mach}} \right) \\
         & \approx & \frac{r}{2n+3}\left(1 + \frac{(2n+1)(2n+3)}{r^2}(\epsilon_n + \epsilon_{\mbox{mach}})\right).
\end{eqnarray}
Thus, we identify the amplication of the relative error as being
\begin{eqnarray}
  \epsilon_{n+1} & = & \frac{(2n+1)(2n+3)}{r^2}(\epsilon_n + \epsilon_{\mbox{mach}})
\end{eqnarray}
This suggests that for $r>2n+1$ that the recursion should be stable.  However, our analysis was just for small $r$.  So let's repeat with the large $r$ expansion for $R_n$ that yields
\begin{eqnarray}
  R_n^{-1} & = & 1 + \frac{n}{r} + \cdots
\end{eqnarray}
hence, using machine arithmetic
\begin{eqnarray}
  R_{n+1} & \approx & (1 + \epsilon_n) \left(1 + \frac{n}{r} \right) - \frac{2n+1}{r} + \epsilon_{\mbox{mach}} \\
         & \approx & 1 - \frac{n+1}{r} + \epsilon_n \left(1 + \frac{n}{r} \right) + \epsilon_{\mbox{mach}} \\
  \epsilon_{n+1} & = & \epsilon_n \left(1 + \frac{n}{r} \right) + \epsilon_{\mbox{mach}}
\end{eqnarray}
Thus, the relative error is still being magnified as we recur up, but now by a factor less than two each iteration.  For $r \gg n$ the accumulation of roundoff error will perhaps be the most significant.  Thus, for very large $r$ the upward recursion can yield many correct significant figures but since it does not ``self-correct'' in the manner of the downward recursion it cannot yield full accuracy without additional techniques?

Scaling reciprocal of $r$ by $(1\pm\epsilon)$ changes in average signed, average absolute, and maximum absolute relative error in units of machine epsilon over the test suite that has $r$ exactly representable in float and above, but most values do not have an exact reciprocal.
\begin{verbatim}
   -1  test bessel2:  8.7695   8.9804  45 eps
    0  test bessel2:  0.917688 1.43011 14 eps 
   +1  test bessel2: -7.52305  7.72235 46 eps
\end{verbatim}

Benefit of gradient correction
\begin{verbatim}
before test bessel2: 0.917688 1.86626 14.756 eps 
 after test bessel2: -0.108574 0.809596 6.00438 eps
\end{verbatim}

\end{document}
